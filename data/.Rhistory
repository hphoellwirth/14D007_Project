terminated <- TRUE
x <- i[[t]]$x
if (x <= 0) { # fumbled in opponent's endzone
x <- 20
}
else if (x > 100) { # opponent's touchdown
x <- 0
r <- -6.8
flag <- FALSE
}
} else { # run
x <- i[[t]]$x - (rpois(1,6) - 2)
if (x > 100) { # safety
terminated <- TRUE
x <- 20
r <- -2.0
}
}
}
# PUNT attempt
else if (u == 'U') {
terminated <- TRUE
x <- i[[t]]$x - (6*rpois(1,10) + 6)
if (x < 0) x <- 20
}
# KICK attempt
else if (u == 'K') {
terminated <- TRUE
if (outcome < max(0, 0.95 - (0.95*(i[[t]]$x))/60)) { # successful field goal
x <- 20
r <- 3
} else { # missed field goal
x <- i[[t]]$x
}
}
if (x <= 0 & flag==TRUE) { # touchdown
terminated <- TRUE
x <- 20
r <- 6.8
}
# increase down
d <- (i[[t]]$d) + 1
# calculate new distance to first down
y <- i[[t]]$y + x - i[[t]]$x
if (y <= 0) { # new first down
y <- min(x,10)
d <- 1
}
if (d > 4) terminated <- TRUE
if (!terminated) {
t <- t + 1
i[[t]] <- list(d=d, x=x, y=y)
}
}
# expected score of opponent's drive
r <- r - ((6.8*x)/100)
return(list(i=i, r=r))
}
generate.sample <- function(mu, Ns=1000) {
D <- list()
for (i in 1:Ns) {
D[[i]] <- simulate.drive(mu)
}
return(D)
}
dummy.heuristic.policy <- function() {
mu <- list()
mu[[1]] <- matrix('P',100,100)
mu[[2]] <- matrix('P',100,100)
mu[[2]][,1:3] <- 'R'
mu[[3]] <- matrix('R',100,100)
mu[[4]] <- matrix('R',100,100)
return(mu)
}
feature.vector <- function(i) {
return(c(0.01*i$x, 0.01*i$y))
}
# translate sample data into data frame for neural network training
df.sample <- function(D, down) {
df <- data.frame()
row <- 1
for (l in 1:length(D)) { # l-th sample trajectory
for (t in 1:length(D[[l]]$i)) { # t-th state
if (D[[l]]$i[[t]]$d == down) {
df[row, 1] <- feature.vector(D[[l]]$i[[t]])[1]
df[row, 2] <- feature.vector(D[[l]]$i[[t]])[2]
df[row, 3] <- D[[l]]$r
row <- row + 1
}
}
}
colnames(df) <- c('x', 'y', 'J')
return(df)
}
# train neural network for particular down
train.nn <- function(D, down, Nt, R) {
df <- df.sample(D, down)
start.weights <- runif((R*2) + 2*R + 1)
start.weights <- rep(0.1, (R*2) + 2*R + 1)
r <- neuralnet(formula = J ~ x + y, data=df, hidden=R, rep=1, startweights=start.weights)
return(r)
}
# train 4 neural networks, one for each down
train.nns <- function(D, Nt=100, R=20) {
r <- list()
for (d in 1:4) {
r[[d]] <- train.nn(D, d, Nt, R)
}
return(r)
}
# estimate reward-to-go from trained neural network
estimate.Js <- function(r) {
J <- list()
df <- expand.grid(seq(1,100), seq(1,100))
for (d in 1:4) {
J[[d]] <- cbind(df, compute(r[[d]], covariate=df)$net.result)
}
return(J)
}
# ----------------------------------------------------------------------
# (3) Dynamic programming algorithm
# ----------------------------------------------------------------------
transition.prob.dummy <- function() {
p <- list()
for (u in 1:4) {
p[[u]] <- matrix(1/(10000*10000), 10000, 10000)
}
return(p)
}
transition.prob <- function(D, mu) {
# initialize transition probabilities
p <- list()
for (u in 1:4) {
p[[u]] <- matrix(0, 10000, 10000)
}
# compute empirical transition probablities
U <- c('P','R','U','K')
transitions <- rep(0,4)
for (l in 1:length(D)) { # l-th sample trajectory
if (length(D[[l]]$i) > 1) {
for (t in 2:length(D[[l]]$i)) { # t-th state
t.i <- D[[l]]$i[[t-1]]
t.j <- D[[l]]$i[[t]]
u <- mu[[t.i$d]][t.i$x, t.j$y] # policy action
u.idx <- which(U==u)
transitions[u.idx] <- transitions[u.idx] + 1
p.i <- t.i$x + (t.i$x - 1) * t.i$y
p.j <- t.j$x + (t.j$x - 1) * t.j$y
p[[u.idx]][p.i,p.j] <- p[[u.idx]][p.i,p.j] + 1
}
}
}
# normalize transition probabilities
for (u in 1:4) {
if (transitions[u] > 0)
p[[u]] <- p[[u]] / transitions[u]
}
return(p)
}
# compute down for particular transition
# transition.down <- function(i, jx, jy) {
#    if (jy == min(i$x,10) & jx < i$x) {
#    }
#}
# get reward-to-go approximation for status i from neural network output
J.approx <- function(Js, i) {
return(Js[[i$d]][Js[[i$d]][,1]==i$x & Js[[i$d]][,2]==i$y,3])
}
update.policy <- function(mu, r, D) {
U <- c('P','R','U','K')
Js <- estimate.Js(r)
p <- transition.prob(D, mu)
for (l in 1:length(D)) { # l-th sample trajectory
for (t in 1:length(D[[l]]$i)) { # t-th state
i <- D[[l]]$i[[t]]
s <- rep(0,4)
for (u in 1:length(U)) {
for(jd in 1:4) {
for (jx in 1:100) {
for (jy in 1:100) {
#cat('.')
p.iju <- p[[u]][i$x+(i$x-1)*i$y, jx+(jx-1)*jy]
if (p.iju > 0)
s[u] <- s[u] + p.iju * J.approx(Js, list(d=jd, x=jx, y=jy))
}
}
}
}
print(s)
mu[[i$d]][i$x,i$y] <- U[which.max(s)]
}
}
return(mu)
}
API <- list(Np=1, Ne=8000, Ns=8000, Nt=8000)
OPI <- list(Np=200, Ne=1, Ns=1, Nt=1)
config <- list(Np=1, Ne=100, Ns=100, Nt=100) # for test purposes
approx.policy.iteration <- function(config) {
i.start <- list(d=1, x=80, y=10)
J <- c()
# 1. initial policy
mu <- list()
mu[[1]] <- dummy.heuristic.policy()
for (k in 1:100) {
# (2.a) obtain estimate for expected reward
if (k %% config$Np == 0) {
J <- c(J, expected.reward(mu[[k]], i.start, config$Ne))
print(expected.reward(mu[[k]], i.start, config$Ne))
}
# (2.b) generate sample trajectories
cat('.')
D <- generate.sample(mu[[k]], config$Ns)
# (2.c) train parameter vector
cat('.')
r <- train.nns(D, config$Nt)
# (2.d) set new policy
cat('.')
mu[[k+1]] <- update.policy(mu[[k]], r, D)
}
return(J)
}
J <- approx.policy.iteration(config)
warnings()
simulate.drive <- function(mu, i.start=NA) {
t <- 1
i <- list()
r <- 0
# initial state
i[[t]] <- i.start
print(i.start)
print(is.na(i.start))
if (is.na(i.start))
i[[t]] <- initial.condition()
terminated <- FALSE
while(!terminated) {
u <- mu[[i[[t]]$d]][i[[t]]$x, i[[t]]$y] # policy action
outcome <- runif(1)
flag <- TRUE
# PASS attempt
if (u == 'P') {
if (outcome < 0.05) { # interception
terminated <- TRUE
x <- i[[t]]$x - (rpois(1,12) - 2)
if (x <= 0) { # intercepted in opponent's end zone
x <- 20
}
else if (x > 100) { # opponent's touchdown
x <- 0
r <- -6.8
flag <- FALSE
}
} else if (outcome < 0.50) { # pass incomplete
x <- i[[t]]$x
} else if (outcome < 0.55) { # sacked
x <- i[[t]]$x + rpois(1,6)
if (x > 100) { # safety
terminated <- TRUE
x <- 20
r <- -2.0
}
} else { # pass complete
x <- i[[t]]$x - (rpois(1,12) - 2)
if (x > 100) { # safety
terminated <- TRUE
x <- 20
r <- -2.0
}
}
}
# RUN attempt
else if (u == 'R') {
if (outcome < 0.05) { # fumble
terminated <- TRUE
x <- i[[t]]$x
if (x <= 0) { # fumbled in opponent's endzone
x <- 20
}
else if (x > 100) { # opponent's touchdown
x <- 0
r <- -6.8
flag <- FALSE
}
} else { # run
x <- i[[t]]$x - (rpois(1,6) - 2)
if (x > 100) { # safety
terminated <- TRUE
x <- 20
r <- -2.0
}
}
}
# PUNT attempt
else if (u == 'U') {
terminated <- TRUE
x <- i[[t]]$x - (6*rpois(1,10) + 6)
if (x < 0) x <- 20
}
# KICK attempt
else if (u == 'K') {
terminated <- TRUE
if (outcome < max(0, 0.95 - (0.95*(i[[t]]$x))/60)) { # successful field goal
x <- 20
r <- 3
} else { # missed field goal
x <- i[[t]]$x
}
}
if (x <= 0 & flag==TRUE) { # touchdown
terminated <- TRUE
x <- 20
r <- 6.8
}
# increase down
d <- (i[[t]]$d) + 1
# calculate new distance to first down
y <- i[[t]]$y + x - i[[t]]$x
if (y <= 0) { # new first down
y <- min(x,10)
d <- 1
}
if (d > 4) terminated <- TRUE
if (!terminated) {
t <- t + 1
i[[t]] <- list(d=d, x=x, y=y)
}
}
# expected score of opponent's drive
r <- r - ((6.8*x)/100)
return(list(i=i, r=r))
}
J <- approx.policy.iteration(config)
simulate.drive <- function(mu, i.start=NA) {
t <- 1
i <- list()
r <- 0
# initial state
i[[t]] <- i.start
if (is.na(i.start))
i[[t]] <- initial.condition()
terminated <- FALSE
while(!terminated) {
u <- mu[[i[[t]]$d]][i[[t]]$x, i[[t]]$y] # policy action
outcome <- runif(1)
flag <- TRUE
# PASS attempt
if (u == 'P') {
if (outcome < 0.05) { # interception
terminated <- TRUE
x <- i[[t]]$x - (rpois(1,12) - 2)
if (x <= 0) { # intercepted in opponent's end zone
x <- 20
}
else if (x > 100) { # opponent's touchdown
x <- 0
r <- -6.8
flag <- FALSE
}
} else if (outcome < 0.50) { # pass incomplete
x <- i[[t]]$x
} else if (outcome < 0.55) { # sacked
x <- i[[t]]$x + rpois(1,6)
if (x > 100) { # safety
terminated <- TRUE
x <- 20
r <- -2.0
}
} else { # pass complete
x <- i[[t]]$x - (rpois(1,12) - 2)
if (x > 100) { # safety
terminated <- TRUE
x <- 20
r <- -2.0
}
}
}
# RUN attempt
else if (u == 'R') {
if (outcome < 0.05) { # fumble
terminated <- TRUE
x <- i[[t]]$x
if (x <= 0) { # fumbled in opponent's endzone
x <- 20
}
else if (x > 100) { # opponent's touchdown
x <- 0
r <- -6.8
flag <- FALSE
}
} else { # run
x <- i[[t]]$x - (rpois(1,6) - 2)
if (x > 100) { # safety
terminated <- TRUE
x <- 20
r <- -2.0
}
}
}
# PUNT attempt
else if (u == 'U') {
terminated <- TRUE
x <- i[[t]]$x - (6*rpois(1,10) + 6)
if (x < 0) x <- 20
}
# KICK attempt
else if (u == 'K') {
terminated <- TRUE
if (outcome < max(0, 0.95 - (0.95*(i[[t]]$x))/60)) { # successful field goal
x <- 20
r <- 3
} else { # missed field goal
x <- i[[t]]$x
}
}
if (x <= 0 & flag==TRUE) { # touchdown
terminated <- TRUE
x <- 20
r <- 6.8
}
# increase down
d <- (i[[t]]$d) + 1
# calculate new distance to first down
y <- i[[t]]$y + x - i[[t]]$x
if (y <= 0) { # new first down
y <- min(x,10)
d <- 1
}
if (d > 4) terminated <- TRUE
if (!terminated) {
t <- t + 1
i[[t]] <- list(d=d, x=x, y=y)
}
}
# expected score of opponent's drive
r <- r - ((6.8*x)/100)
return(list(i=i, r=r))
}
D <- generate.sample(mu[[k]], config$Ns)
mu <- list()
mu[[1]] <- dummy.heuristic.policy()
D <- generate.sample(mu[[k]], config$Ns)
k <- 1
D <- generate.sample(mu[[k]], config$Ns)
Nt <- 100
R <- 20
r <- list()
for (d in 1:4) {
r[[d]] <- train.nn(D, d, Nt, R)
}
J <- list()
df <- expand.grid(seq(1,100), seq(1,100))
for (d in 1:4) {
J[[d]] <- cbind(df, compute(r[[d]], covariate=df)$net.result)
}
J <- approx.policy.iteration(config)
J <- list()
df <- expand.grid(seq(1,100), seq(1,100))
for (d in 1:4) {
J[[d]] <- cbind(df, compute(r[[d]], covariate=df)$net.result)
}
J <- list()
df <- expand.grid(seq(1,100), seq(1,100))
for (d in 1:4) {
J[[d]] <- cbind(df, compute(r[[d]], covariate=df)$net.result)
}
U <- c('P','R','U','K')
Js <- estimate.Js(r)
p <- transition.prob(D, mu)
mu <- mu[[1]]
p <- transition.prob(D, mu)
for (l in 1:length(D)) { # l-th sample trajectory
for (t in 1:length(D[[l]]$i)) { # t-th state
i <- D[[l]]$i[[t]]
s <- rep(0,4)
for (u in 1:length(U)) {
for(jd in 1:4) {
for (jx in 1:100) {
for (jy in 1:100) {
#cat('.')
p.iju <- p[[u]][i$x+(i$x-1)*i$y, jx+(jx-1)*jy]
if (p.iju > 0)
s[u] <- s[u] + p.iju * J.approx(Js, list(d=jd, x=jx, y=jy))
}
}
}
}
print(s)
mu[[i$d]][i$x,i$y] <- U[which.max(s)]
}
}
i.start <- list(d=1, x=80, y=10)
J <- c()
mu <- list()
mu[[1]] <- dummy.heuristic.policy()
config <- list(Np=1, Ne=100, Ns=100, Nt=100) # for test purposes
if (k %% config$Np == 0) {
J <- c(J, expected.reward(mu[[k]], i.start, config$Ne))
print(expected.reward(mu[[k]], i.start, config$Ne))
}
D <- generate.sample(mu[[k]], config$Ns)
r <- train.nns(D, config$Nt)
mu[[k+1]] <- update.policy(mu[[k]], r, D)
mu[[k+1]] <- update.policy(mu[[k]], r, D)
mu <- mu[[1]]
U <- c('P','R','U','K')
Js <- estimate.Js(r)
Js <- estimate.Js(r)
str(r)
D
?nnet
install.packages("nnet")
simulate.drive <- function(mu, i.start=NA) {
simulate.drive <- function(mu, i.start=NA) {
